"""Contain base class for OData implementations"""
import datetime
import typing
from maas_collector.rawdata.collector.http.abstract_query_strategy import (
    AbstractHttpQueryStrategy,
)
from maas_collector.rawdata.collector.httpmixin import HttpMixin
from maas_collector.rawdata.collector.journal import (
    CollectorJournal,
    CollectorReplayJournal,
)

from maas_model import datetime_to_zulu
from requests.exceptions import RequestException


class AbstractODataQueryStrategy(AbstractHttpQueryStrategy):
    """Base class for OData query implementation"""

    FILE_NAME_PATTERN = None

    def __init__(
        self,
        collector: "ODataCollector",
        config: "ODataCollectorConfiguration",
        http_session: HttpMixin,
        start_date: datetime.datetime,
        end_date_without_offset: datetime.datetime,
        journal: typing.Union[CollectorJournal, CollectorReplayJournal],
    ):
        """Perform query initialization using first the default HTTP query init
        then overload with specific odata query configuration

        Args:
            collector (ODataCollector): collector object
            config (ODataCollectorConfiguration): configuration of collector
            http_session (HttpMixin): Initialized http session
            start_date (datetime.datetime): start date of collection
            end_date_without_offset (datetime.datetime): end date of collection
            journal (typing.Union[CollectorJournal, CollectorReplayJournal]):
                        journal used to check if collection shall start or
                        if data to collect are too old or parsed too recently
        """
        super().__init__(
            collector,
            config,
            http_session,
            start_date,
            end_date_without_offset,
            journal,
        )

        if config.odata_start_offset:
            self.start_date = self.start_date - datetime.timedelta(
                minutes=config.odata_start_offset
            )

        self.product_per_page = config.product_per_page
        self.odata_query_filter = config.odata_query_filter

        self._iter_start_date = self.start_date
        self._iter_end_date = min(
            self.start_date + datetime.timedelta(minutes=self.config.max_time_window),
            self.end_date,
        )
        self._skip_number = 0

    def get_filename(self, page: int) -> str:
        """get file name of a page for download"""
        return self.FILE_NAME_PATTERN.format(
            interface_name=self.interface_name,
            start_date=self.start_date.strftime("%Y%m%dT%H%M%S"),
            end_date=self.end_date.strftime("%Y%m%dT%H%M%S"),
            product_per_page=self.product_per_page,
            current_page=page,
        )

    def __iter__(self):
        """yields pages of product

        Args:
            args (dict): query arguments

        Raises:
            NotImplemented: abstract methods
        """
        total_entities = 0

        current_query_index = 0

        self.logger.debug(
            "[__ITER__][MAIN] FROM %s TO %s", self.start_date, self.end_date
        )

        while self._iter_start_date < self.end_date:
            if self.collector.should_stop_loop:
                break

            self.logger.debug(
                "[__ITER__][LOCAL] FROM %s TO %s",
                self._iter_start_date,
                self._iter_end_date,
            )

            self.logger.debug(
                "Query nÂ°%d for %s", current_query_index, self.interface_name
            )

            try:
                entities = self._get_entities(self._skip_number)

            except RequestException as connection_error:
                self.logger.error(
                    "[%s][SKIP] Request Error querying products : %s -> %s",
                    self.interface_name,
                    self.product_url,
                    connection_error,
                )
                break
            except ValueError as value_error:
                self.logger.error(
                    "[%s][SKIP] Value error querying products : %s -> %s",
                    self.interface_name,
                    self.product_url,
                    value_error,
                )
                break

            yield entities

            nb_entities = self._count_item_payload(entities)

            total_entities += nb_entities

            self.logger.debug(
                "[__ITER__][LOCAL][STATS] Collected %s (psize %s) entities",
                nb_entities,
                self.product_per_page,
            )

            if self.collector.get_pending_document_count() == 0:
                # preserve time window so it can continue after restart, as no callback
                # will set the last date.
                self.logger.debug(
                    "Keep time window start in journal: %s", self._iter_start_date
                )
                self.journal.document.last_date = self._iter_start_date
                self.journal.document.save(refresh=True)

            if nb_entities < self.product_per_page:
                # Last iter on the current timerange

                self._iter_start_date = self._iter_end_date
                self._iter_end_date = min(
                    self._iter_start_date
                    + datetime.timedelta(minutes=self.config.max_time_window),
                    self.end_date,
                )

                self._skip_number = 0

                self.logger.debug(
                    "[__ITER__][LOCAL] NEXT FROM %s TO %s",
                    self._iter_start_date,
                    self._iter_end_date,
                )
            else:
                # keep iter in the same time range
                # move start page to continue to collect first page (no skip)

                # if a full page with a same doi use skip
                # delay with journal

                if self.journal.last_date <= self._iter_start_date:
                    self._skip_number += 1
                    self.logger.debug(
                        "[__ITER__][LOCAL] _iter forward last_date, increase skip number (%s) %s TO %s (J: %s)",
                        self._skip_number,
                        self._iter_start_date,
                        self._iter_end_date,
                        self.journal.last_date,
                    )
                else:
                    self._skip_number = 0
                    self.logger.debug(
                        "[__ITER__][LOCAL](OPTI) _iter backward last_date %s TO %s (J: %s) (%s)",
                        self._iter_start_date,
                        self._iter_end_date,
                        self.journal.last_date,
                        self._skip_number,
                    )
                    self._iter_start_date = self.journal.last_date

            current_query_index += 1

        self.logger.debug("[__ITER__][MAIN][END]")
        self.logger.info(
            "[%s] Finally retrieve %d products in %s queries",
            self.interface_name,
            total_entities,
            current_query_index,
        )

    def _get_entities(self, page_number):
        """Get entities from the url in config between the start_date and the end_date

        Args:
            page : page of entities

        Raises:
            ValueError: If no status code 200

        Returns:
            Object: an object who contains the json response between the given date
        """
        self.logger.debug(
            "Retrieve entities between %s and %s from %s page %s",
            self._iter_start_date,
            self._iter_end_date,
            self.product_url,
            page_number,
        )

        query_config = self.odata_query_filter.format(
            publication_start_date=self._format_date(self._iter_start_date),
            publication_end_date=self._format_date(self._iter_end_date),
        )

        full_query = (
            f"$filter={query_config}"
            f"&$orderby={self.config.odata_query_order_by}"
            f"&$top={self.product_per_page}"
            f"&$skip={self.product_per_page * page_number}"
            f"{self.config.custom_query_suffix}"
        )

        query_url = (
            f"{self.product_url}{self.config.odata_entity_location}"
            f"{self.config.odata_entities}?{full_query}"
        )

        self.logger.debug("URL : %s", query_url)

        headers = self.authentication.get_headers()

        response = self.http_session.get(
            query_url, headers=headers, timeout=self.collector.http_config.timeout
        )

        if not 200 <= response.status_code < 300:
            # serious problem
            self.logger.error(
                "Error querying %s %d: %s",
                query_url,
                response.status_code,
                response.content,
            )
            raise ValueError(f"Error querying {query_url}")

        return self._deserialize_response(response)

    def _format_date(self, date):
        """Default method to insert date in odata query

        Args:
            date (datetime): datetime

        Returns:
            str: the datetime stringify for REST operations
        """
        return datetime_to_zulu(date)

    def _deserialize_response(self, response):
        raise NotImplementedError()

    def _count_item_payload(self, payload):
        raise NotImplementedError()
